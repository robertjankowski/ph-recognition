{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ph recognition based on the image color\n",
    " ***\n",
    " \n",
    " ### I downloaded ph-scale images from [link](https://www.shutterstock.com/search/ph+scale). Then I split every image to 15 small ones and I labeled it based on the ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # for images\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "data_dir = 'C:\\\\Users\\\\asus\\\\Desktop\\\\data_science_python\\\\ph_project\\\\ph_datasets\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_COUNT = 15\n",
    "images_list = []\n",
    "PH_DATASETS = 7\n",
    "# add image to list in format [blue, green, red, label=ph]\n",
    "for j in range(PH_DATASETS):\n",
    "    for i in range(PH_COUNT):\n",
    "        image = cv2.imread(data_dir + 'ph{}\\\\{}.jpg'.format(j+1, i))\n",
    "        images_list.append([image[3][3][0],image[3][3][1],image[3][3][2],i])\n",
    "\n",
    "# transform data to pandas DataFrame\n",
    "df = pd.DataFrame(np.asarray(images_list),columns=['blue','green','red','label'])\n",
    "df = shuffle(df)\n",
    "# df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8\n",
      "  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2\n",
      "  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11\n",
      " 12 13 14  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  1  2  3  4  5\n",
      "  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df['label']) \n",
    "X = np.array(df.drop(['label'], axis=1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94, 3), (11, 3), (94,), (11,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIZE = X.shape[0]\n",
    "TRAIN_SIZE = int(SIZE*0.9)\n",
    "train_x = X[:TRAIN_SIZE]\n",
    "train_y = y[:TRAIN_SIZE]\n",
    "test_x = X[TRAIN_SIZE:]\n",
    "test_y = y[TRAIN_SIZE:]\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing machine learning algorithms\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# clf = GradientBoostingRegressor()\n",
    "# clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingRegressor with tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_features must be in (0, n_features]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-92b096658e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m clf = GradientBoostingRegressor(loss='lad', learning_rate=0.2,\n\u001b[0;32m      2\u001b[0m                                max_depth=4, max_features=15)\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python3.6.5\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python3.6.5\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python3.6.5\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 788\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python3.6.5\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python3.6.5\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n",
      "\u001b[1;31mValueError\u001b[0m: max_features must be in (0, n_features]"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingRegressor(loss='lad', learning_rate=0.2,\n",
    "                               max_depth=4,)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 12.84, labels 14\n",
      "Predict 1.00, labels 1\n",
      "Predict 1.00, labels 1\n",
      "Predict 13.15, labels 14\n",
      "Predict 1.69, labels 2\n",
      "Predict 6.90, labels 7\n",
      "Predict 1.97, labels 3\n",
      "Predict 12.70, labels 12\n",
      "Predict 8.41, labels 9\n",
      "Predict 11.68, labels 12\n",
      "Predict 12.70, labels 12\n",
      "0.2727272727272727\n",
      "0.9834026776421096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# testing \n",
    "accuracy = 0\n",
    "for i in range(len(test_x)):\n",
    "    y_predict = clf.predict([test_x[i]])\n",
    "    print('Predict {:.2f}, labels {}'.format(y_predict[0],test_y[i]))\n",
    "    # add some margin\n",
    "    if(abs(y_predict[0] - test_y[i]) <= 0.3):\n",
    "        accuracy += 1\n",
    "accuracy = float(accuracy/len(test_x))\n",
    "print(accuracy)\n",
    "# return the mean accuracy on the given test data and labels\n",
    "r2 = r2_score(test_y, clf.predict(test_x))\n",
    "print(r2)\n",
    "# overfitting !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gradient_regression_85.pickle']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model \n",
    "joblib.dump(clf,'saved_models//gradient_regression_85.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle save model\n",
    "number = 2\n",
    "model_pickle = open('saved_models//model-{:.2f}-v{}.pickle'.format(r2, number),'wb')\n",
    "pickle.dump(clf, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
